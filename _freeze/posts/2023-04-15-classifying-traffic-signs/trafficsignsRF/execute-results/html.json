{
  "hash": "67ab62b3f1c7181f22e19ee1f242e72e",
  "result": {
    "markdown": "---\ntitle: Classifying Traffic Signs with Machine Learning (Part 1)\ndescription: Random Forest begins\nimage: sign_predictor.gif\ncache: true\nwarning: false\n---\n\nThis post is part 1 of the three-part series on classifying traffic signs with machine learning. In this part, I will show you how to preprocess images of traffic signs, reshape those images and use them with random forest algorithm for classification. Part of image pre-processing and visualization code comes from [*The Complete Self Driving Car Course* github repo](https://github.com/PacktPublishing/The-Complete-Self-Driving-Car-Course---Applied-Deep-Learning) which has a MIT license [^longnote].  \n\n[^longnote]: MIT License\n\n    Copyright (c) 2019 Packt\n\n    Permission is hereby granted, free of charge, to any person obtaining a copy\n    of this software and associated documentation files (the \"Software\"), to deal\n    in the Software without restriction, including without limitation the rights\n    to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n    copies of the Software, and to permit persons to whom the Software is\n    furnished to do so, subject to the following conditions:\n\n    The above copyright notice and this permission notice shall be included in all\n    copies or substantial portions of the Software.\n\n    THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n    IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n    FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n    AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n    LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n    OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n    SOFTWARE.\n    \n\nRandom Forest (RF) is generally believed to be a useful algorithm for regression and classification problems where training data is tabular. In this post though, I will be using RF to predict the class of traffic signs that are available as images. Therefore, I would first reshape the input images to tabular form as a large 2-dimensional array (matrix). In the later posts, I will use convolution filters for a neural network as well as RF again to make predictions. \n\nLet's begin by loading the required packages.  \n\n![](begin.gif)\n\n\n### Load packages\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nimport pickle\nimport pandas as pd\nimport cv2\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n```\n:::\n\n\n### Load data\n\nI use the `pickle` package to load the training and test data. These two sets come from the [german traffic signs bitbucket repo](https://bitbucket.org/jadslim/german-traffic-signs). The repo also contains a validation dataset, but I won't be using it as I'm not doing any hyperparameter tuning in this post.  \n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nwith open('german-traffic-signs/train.p', 'rb') as f:\n    train_data = pickle.load(f)\n\nwith open('german-traffic-signs/test.p', 'rb') as f:\n    test_data = pickle.load(f)\n```\n:::\n\n\n### What does the data look like?\n\nTraining data is a dictionary that has the following components:  \n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ntrain_data.keys()\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\ndict_keys(['coords', 'labels', 'features', 'sizes'])\n```\n:::\n:::\n\n\nI only need the `features` (images) and their corresponding class `labels`. There are a total of 43 traffic sign types in these data. In the following piece of code, I rename the `features` and `labels` as training and test set pairs:  \n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# Split out features and labels\nX_train, y_train = train_data['features'], train_data['labels']\nX_test, y_test = test_data['features'], test_data['labels']\n\n# 4 dimensional\nprint(\"X_train shape:\\n\\n\", X_train.shape, \"\\n\")\nprint(\"X_test shape:\\n\\n\", X_test.shape)\n\nprint(\"=========\\n=========\\n\")\n\nprint(\"y_train shape:\\n\\n\", y_train.shape, \"\\n\")\nprint(\"y_test shape:\\n\\n\", y_test.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nX_train shape:\n\n (34799, 32, 32, 3) \n\nX_test shape:\n\n (12630, 32, 32, 3)\n=========\n=========\n\ny_train shape:\n\n (34799,) \n\ny_test shape:\n\n (12630,)\n```\n:::\n:::\n\n\nWe can see that the training data and test data contain more than 34K and 12K images respectively. Moreover, these images are sized as `32 x 32` making a total of `32 x 32 = 1,024` pixels per image. The last number, `3`, in the `shape` attribute indicates that these images are coloured with 3 channels of red, green and blue.  \n\n#### Visualizing data\n\nImages are made of pixels that contain values from 0 (black) to 255 (white). The traffic sign images have 3 channels, therefore, each channel contains arrays of pixel intensity values. Following shows the first channel of a single image randomly picked from the training data:  \n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nnp.random.seed(42)\nnum_images = 5\nrandom_indices = np.random.choice(X_train.shape[0], num_images, replace=False)\nrandom_images = X_train[random_indices]\nrandom_images_classes = y_train[random_indices]\n\nfirst_image = random_images[0]\nfirst_image_class = random_images_classes[0]\n\nnp.set_printoptions(threshold=np.inf, linewidth=np.inf)\nprint(first_image[:,:,0])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[ 89  86  86  84  84  83  74  67  75  81  75  90  87  70  75  86 103  88  81  77  69  71  64  59  65  64  60  50  47  61  74  63]\n [ 86  88  83  80  82  83  81  69  79  73  70  88  83  73  75  87 103  83  73  73  65  72  73  66  71  74  67  56  47  47  59  60]\n [101 104  87  83  83  79  78  71  79  73  81  88  83  74  75  87 101  80  73  71  65  63  66  68  71  75  70  64  58  56  59  61]\n [101  98  87  90  85  76  68  72  79  81  96 104  94  77  86 119 122  88  78  70  64  63  62  64  65  64  65  68  71  71  62  62]\n [ 90  89  96  96  84  75  68  76  90  92  93  97  93  82 126 168 164 110  84  75  62  61  57  57  59  54  57  69  76  75  65  59]\n [ 82  82  96  96  82  77  77  86 104  95  77  70  72  94 170 170 195 148  94  75  63  54  52  58  56  47  59  83  91  82  67  55]\n [ 85  90 103  97  81  77  78  82  93  88  82  70  86 148 174 167 187 188 108  88  71  51  49  57  56  53  67  78  88  78  72  58]\n [ 77  86  97  86  79  80  79  83  86  84  82  79 111 193 180 173 172 185 140  93  78  53  48  59  65  57  63  63  64  68  77  63]\n [ 70  64  75  66  69  79  84  88  94  97  88  99 155 188 170 175 166 181 187  99  76  58  54  62  69  59  54  60  58  60  66  66]\n [ 73  57  57  64  63  70  79  84 101 102  83 119 182 172 172 226 194 173 191 136  83  63  61  63  70  69  53  71  69  62  62  76]\n [ 75  57  59  70  69  79  85  78  85  85  81 151 196 178 211 251 228 171 174 187 128  78  66  65  70  76  52  54  65  71  67  69]\n [ 70  55  58  68  85  98  91  85  85  77 115 183 186 187 239 255 247 196 173 190 162  92  68  58  57  69  58  54  64  71  68  66]\n [ 74  53  56  67  87  99  92  91  85  78 148 193 182 212 253 255 255 233 182 172 195 127  67  55  51  52  56  60  68  72  67  63]\n [ 76  56  60  73  74  81  85  88  76  98 178 187 186 237 253 238 247 252 218 173 183 166  87  65  64  60  65  70  75  77  75  62]\n [ 70  55  70  88  75  77  96  93  95 164 193 175 204 251 229 144 191 248 244 194 178 196 140  75  67  70  67  76  77  72  79  73]\n [ 76  67  77  86  76  81  95  89 116 191 190 185 232 241 140  65  94 189 249 223 181 194 180  94  61  71  68  76  80  72  78  70]\n [ 81  86  87  87  83  84  91 102 175 206 184 208 250 214  77  38  37 121 242 248 197 170 189 157  87  79  77  77  80  78  80  71]\n [ 76  87  91  90  97  97  92 128 210 200 191 236 255 206  70  40  38 109 237 253 231 177 179 190 130  88  83  81  80  78  84  76]\n [ 64  78  81  85 108 111  97 153 206 189 212 245 231 176  66  41  40  81 162 189 242 204 170 192 183 102  88  85  82  79  87  77]\n [ 65  69  88  88  98  98 106 196 194 182 232 210 135 111  58  42  39  67 123 161 240 238 183 182 205 135  94  77  75  80  88  81]\n [ 65  70  91  91  93 103 143 199 185 202 250 235 201 170  66  42  41  94 210 235 249 254 214 171 182 184 121  73  77  90  92  93]\n [ 71  80  88  89  93 130 192 196 188 229 255 254 253 224  77  49  48  95 233 255 255 255 240 187 172 196 148  84  81  94  98 105]\n [ 84  96  95  85  97 167 211 186 206 249 255 255 255 226  90 117 111 103 232 255 255 255 253 218 173 181 187 110  73  87  90  97]\n [ 84  99 101  93 121 189 195 192 235 255 255 255 255 233 159 216 217 176 238 255 255 255 255 241 194 184 190 163  82  83  83  84]\n [ 81  97 104 102 171 233 209 215 248 255 255 255 255 251 242 253 255 247 252 255 255 255 255 247 208 196 166 191 135  95  86  81]\n [ 88  97 103 123 199 238 241 245 254 250 244 240 236 235 232 227 224 221 221 220 215 215 225 228 203 194 160 196 183 109  88  79]\n [ 83  96 109 159 195 210 244 247 229 216 208 206 201 201 199 194 189 187 187 187 180 181 196 205 211 196 172 172 189 120  79  71]\n [ 68  93 115 170 184 192 230 249 237 214 202 201 199 199 194 190 186 186 188 191 191 203 213 213 216 207 197 192 174 102  75  79]\n [ 76 101 114 154 196 205 212 231 219 197 185 183 189 209 182 164 161 168 173 162 145 147 147 143 147 148 142 136 112  77  75  84]\n [101 123 142 165 193 179 160 165 138 105  87  84  98 139  97  79  89 107 124  95  65  64  70  75  82  88  82  74  66  66  76  85]\n [114 143 183 194 178 144 144 159 131  83  61  63  89 125  86  73  88 109 142 110  73  68  72  77  79  82  76  74  73  76  84  91]\n [104 127 175 187 155 142 169 183 149  85  54  56 107 174 105  75  89 106 124  98  82  73  64  57  64  74  69  73  75  82  96  89]]\n```\n:::\n:::\n\n\nDo you notice the triangle shape the numbers make? Let's see the actual image:  \n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nplt.imshow(random_images[0]);\nplt.axis('off');\n```\n\n::: {.cell-output .cell-output-display}\n![](trafficsignsRF_files/figure-html/cell-7-output-1.png){width=389 height=389}\n:::\n:::\n\n\nThe german traffic signs repo also contains a table with the sign label and description. Following shows the first few rows:  \n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\ndata = pd.read_csv('german-traffic-signs/signnames.csv')\ndata.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ClassId</th>\n      <th>SignName</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Speed limit (20km/h)</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Speed limit (30km/h)</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Speed limit (50km/h)</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Speed limit (60km/h)</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>Speed limit (70km/h)</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nUsing the table above, I plot 5 images randomly picked from the training data and show each channel as follows:  \n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nrandom_images_classes_namez = np.array(data.loc[data.ClassId.isin(random_images_classes), ['SignName']])\n```\n:::\n\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nplot = plt.figure()\nplot.set_figwidth(10)\nplot.set_figheight(8)\n# plot each channel for the random images\nfor i in range(num_images):\n    for j in range(3):\n        plt.subplot(num_images, 3, i*3+j+1)\n        if j == 0: # Red channel\n            plt.imshow(random_images[i][:,:,j], cmap='Reds')\n        elif j == 1: # Green channel\n            plt.imshow(random_images[i][:,:,j], cmap='Greens')\n        elif j == 2: # Blue channel\n            plt.imshow(random_images[i][:,:,j], cmap='Blues')\n        plt.axis('off')\n        plt.title('{} {}'.format(random_images_classes_namez[i][0], j+1))\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](trafficsignsRF_files/figure-html/cell-10-output-1.png){width=853 height=631}\n:::\n:::\n\n\nHopefully, for people new to image classification, it is clear what we are dealing with. Let's also look at the top five traffic signs in the training data:  \n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nnum_of_samples=[]\nnamez=[]\n\ncols = 5\nnum_classes = 43\n\n\n\nfor i, row in data.iterrows():\n  x_selected = X_train[y_train == i]\n  num_of_samples.append(len(x_selected))\n  namez.append(data.loc[i, 'SignName'])\n  \n# print(num_of_samples)\n# print(namez)\n# print(len(num_of_samples))\n# print(len(namez))\n\nnum_of_samples = np.array(num_of_samples)\nnamez = np.array(namez)\n\nind1 = np.argpartition(num_of_samples, -10)[-10:]\nind1 = ind1[np.argsort(num_of_samples[ind1])]\nnum_of_samples_top5 = num_of_samples[ind1]\nnamez_top5 = namez[ind1]\n\nplt.barh(namez_top5, num_of_samples_top5);\nplt.title(\"Distribution of the training dataset\");\nplt.ylabel(\"\");\nplt.xlabel(\"Number of images\");\nplt.show();\n```\n\n::: {.cell-output .cell-output-display}\n![](trafficsignsRF_files/figure-html/cell-11-output-1.png){width=844 height=449}\n:::\n:::\n\n\nSpeed limit, no passing and yield signs are some of the most common signs in this training data. Having unequal number of images can affect the performance of a machine learning model as it is biased by the most frequent classes. But it is a topic for another day.  \n\n### Preprocessing data\n\nNext, I preprocess the data as follows:  \n\n1. Convert the original coloured image to a grayscale image. This would reduce the required computing resources as 3 channels are reduced to a single channel.  \n2. Equalize the intensities of the image.  \n3. Standardize the image by dividing each pixel intensity with 255.  \n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\ndef grayscale(img):\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    return img\n  \ndef equalize(img):\n    img = cv2.equalizeHist(img)\n    return img\n  \ndef preprocess(img):\n    img = grayscale(img)\n    img = equalize(img)\n    img = img/255\n    return img\n  \nX_train = np.array(list(map(preprocess, X_train)))\nX_test = np.array(list(map(preprocess, X_test)))\n```\n:::\n\n\n#### Reshaping images\n\nIf I were to use a convolutional neural network, I would use the data as individual arrays of images, similar to how it is now. But RF needs the data to be in a tabular form, i.e., rows and columns for all data. Therefore, I reshape the data in a format where each row contains `32 x 32 = 1,024` columns of a single image:  \n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nnum_pixels = (X_train.shape[1] * X_train.shape[2])\n\nX_train = X_train.reshape(X_train.shape[0], num_pixels)\nX_test = X_test.reshape(X_test.shape[0], num_pixels)\n\n\nprint(X_train.shape)\nprint(X_test.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(34799, 1024)\n(12630, 1024)\n```\n:::\n:::\n\n\nThe numbers above show the rows and columns of training and test data.\n\n### Machine Learning\n\nWe are ready to fit a RF to the training data now. I use default values of all parameters but provide a `random_state` to make sure the results can be replicated. I also use `n_jobs=-1` to take advantage of all cores of my system.   \n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nclf = RandomForestClassifier(random_state=0,  n_jobs=-1)\nclf.fit(X_train, y_train)\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```{=html}\n<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(n_jobs=-1, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(n_jobs=-1, random_state=0)</pre></div></div></div></div></div>\n```\n:::\n:::\n\n\nThe model is fit and is now ready to make predictions. I make predictions on both training and test sets.  \n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\ny_pred_train = clf.predict(X_train)\ny_pred_test = clf.predict(X_test)\n```\n:::\n\n\nAnd estimate the accuracy of predictions:  \n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\naccuracy_train = accuracy_score(y_train, y_pred_train)\nprint(\"Train Accuracy:\", np.round(accuracy_train))\n\naccuracy_test = accuracy_score(y_test, y_pred_test)\nprint(\"Test Accuracy:\", np.round(accuracy_test))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTrain Accuracy: 1.0\nTest Accuracy: 1.0\n```\n:::\n:::\n\n\nWe can see that the test accuracy is significantly lower than the train accuracy. The model seem to be overfitting. With some hyperparameter tuning, the model may show better results. However, keep in mind that this is expected as without learning anything about the specific properties of a traffic sign (that convolutional layers can extract), RF is not able to learn much from the raw pixel intensities.  \n\n### Predicting a new image\n\nLet's see how our RF model do on a stop sign image downloaded from the internet. Following is the image:  \n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\nimport requests\nfrom PIL import Image\n# url = 'https://c8.alamy.com/comp/A0RX23/cars-and-automobiles-must-turn-left-ahead-sign-A0RX23.jpg'\n# r = requests.get(url, stream=True)\nimg = Image.open('STOP_sign.jpg')\nplt.imshow(img, cmap=plt.get_cmap('gray'))\n```\n\n::: {.cell-output .cell-output-display execution_count=16}\n```\n<matplotlib.image.AxesImage at 0x2911106c8b0>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](trafficsignsRF_files/figure-html/cell-17-output-2.png){width=405 height=416}\n:::\n:::\n\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\ndef preprocess2(img):\n  img = np.asarray(img)\n  img = cv2.resize(img, (32, 32))\n  img = grayscale(img)\n  img = equalize(img)\n  img = img/255\n  img = img.reshape(1, 1024)\n  return img\n\nimg = preprocess2(img)\n```\n:::\n\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\nprint(\"Predicted sign: \"+ str(clf.predict(img)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPredicted sign: [14]\n```\n:::\n:::\n\n\n",
    "supporting": [
      "trafficsignsRF_files\\figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}